{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Visualizing Data ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_layer_size  = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "print('Loading and Visualizing Data ...\\n')\n",
    "mat_contents = sio.loadmat('ex4data1.mat')\n",
    "X = mat_contents['X']\n",
    "y = mat_contents['y']\n",
    "m = len(y)\n",
    "#print(m)\n",
    "\n",
    "rand_indices = np.random.permutation(m)\n",
    "#print(rand_indices)\n",
    "rand_indices = rand_indices.reshape(-1, 1)\n",
    "sel = X[rand_indices[0:100, :]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrangeParams(t1, t2):\n",
    "    #print(t1.shape)\n",
    "    #print(t2.shape)\n",
    "    return np.concatenate((t1.reshape(t1.size, 1, order='F'),\n",
    "                           t2.reshape(t2.size, 1, order='F'))\n",
    "                          , axis=0).flatten()\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayData(X):\n",
    "    fig, ax = plt.subplots(10,10,sharex=True,sharey=True)\n",
    "    img_num = 0\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            # Convert column vector into 20x20 pixel matrix\n",
    "            # You have to transpose to display correctly\n",
    "            img = X[img_num,:].reshape(20,20).T\n",
    "            ax[i][j].imshow(img,cmap='gray')\n",
    "            img_num += 1\n",
    "\n",
    "    return (fig, ax)\n",
    "    \n",
    "#figure, ax = displayData(sel)\n",
    "#figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Saved Neural Network Parameters ...\n",
      "\n",
      "(25, 401)\n",
      "(10, 26)\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading Saved Neural Network Parameters ...\\n')\n",
    "nn_contents = sio.loadmat('ex4weights.mat')\n",
    "\n",
    "Theta1 = nn_contents['Theta1'] \n",
    "Theta2 = nn_contents['Theta2']\n",
    "print(Theta1.shape)\n",
    "print(Theta2.shape)\n",
    "\n",
    "nn_params = arrangeParams(Theta1, Theta2)\n",
    "#print(Theta1[0][1])\n",
    "#print(Theta2[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunctionPrateek(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamb):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    X = np.c_[np.ones(m),X]\n",
    "    J=0\n",
    "    Theta1 = nn_params[:(hidden_layer_size*(input_layer_size+1))].reshape((input_layer_size+1),hidden_layer_size).T\n",
    "    Theta2 = nn_params[-(num_labels*(hidden_layer_size+1)):].reshape(num_labels,(hidden_layer_size+1))\n",
    "#     print(\"Theta1 shape \", Theta1.shape)\n",
    "#     print(\"Theta2 shape \", Theta2.shape)\n",
    "#     print(\"X shape \", X.shape)\n",
    "    a1 = sigmoid(X.dot(Theta1.T)) # X = 5000* 401 , Theta1.T =401*25 a1= 5000*25\n",
    "    assert(a1.shape==(X.shape[0],Theta1.shape[0]))\n",
    "    a1 = np.c_[np.ones(m), a1] # add ones in a1 so a1.shape = 5000*26\n",
    "    h= sigmoid(a1.dot(Theta2.T)) # a1=5000*26 and Theta2 =10*26 so h = 5000*10\n",
    "    assert(h.shape == (a1.shape[0],Theta2.shape[0]))\n",
    "    for i in range(1,num_labels+1):\n",
    "        yk = (y==i)*1\n",
    "        \n",
    "        h_of_x = (h[:,i-1].reshape(-1,1))\n",
    "        J = J - (np.sum((yk*np.log(h_of_x)) + ((1-yk)*np.log(1-h_of_x))))/m\n",
    "    \n",
    "    \n",
    "    rtheta1=np.sum(np.square(Theta1[:,1:]))\n",
    "    rtheta2=np.sum(np.square(Theta2[:,1:]))\n",
    "    bias = lamb/(2*m)\n",
    "    J= J+(bias*(rtheta1+rtheta2));\n",
    "    Del1=0\n",
    "    Del2=0\n",
    "    for t in range(1,m+1):\n",
    "        A1 = X[t-1:t].T\n",
    "        Z2 = Theta1.dot(A1)\n",
    "        a = np.array([1]).reshape(-1,1)\n",
    "        A2 = np.concatenate((a,sigmoid(Z2)), axis=0)\n",
    "        Z3 = Theta2.dot(A2)\n",
    "        H = sigmoid(Z3)\n",
    "        actual = y[t-1];\n",
    "        yk= np.zeros((num_labels,1))\n",
    "        yk[actual-1] = 1;\n",
    "        del3 = H - yk;\n",
    "        del2 =(Theta2[:,1:].T.dot(del3)) * sigmoidGradient(Z2)\n",
    "        Del1 = Del1 + del2 * A1.T\n",
    "        Del2 = Del2 + del3 * A2.T\n",
    "\n",
    "    Theta1_grad = (Del1/m) + (lamb/m) * np.concatenate((np.zeros((hidden_layer_size,1)),Theta1[:,1:]), axis=1)\n",
    "    Theta2_grad = (Del2/m) + (lamb/m) *  np.concatenate((np.zeros((num_labels,1)), Theta2[:,1:]), axis=1)\n",
    "\n",
    "    grad = np.concatenate((Theta1_grad.T.reshape(Theta1_grad.size,1), Theta2_grad.reshape(Theta2_grad.size,1)), axis=0)  \n",
    "    grad = grad.reshape(-1)\n",
    "    print(J, grad.shape)\n",
    "    return J,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2876291651613189 (10285, 1)\n",
      "['Cost at parameters (loaded from ex4weights): \\n(this value should be about 0.287629)\\n'] 0.2876291651613189\n"
     ]
    }
   ],
   "source": [
    "def nnCostFunction(nn_params_l, input_layer_size_l, hidden_layer_size_l, num_labels_l, X_l, y_l, lambda_l):\n",
    "   \n",
    "    Theta1_l = nn_params_l[0:(hidden_layer_size_l * (input_layer_size_l + 1))].reshape(input_layer_size_l + 1, hidden_layer_size_l).T\n",
    "    Theta2_l = nn_params_l[(Theta1_l.size):nn_params_l.size].reshape(hidden_layer_size_l + 1, num_labels_l).T\n",
    "   \n",
    "    m = X_l.shape[0] #5000X400    \n",
    "    X_l = np.c_[np.ones((m, 1)), X_l] #5000X401\n",
    "   \n",
    "    z1 = X_l.dot(Theta1_l.T)\n",
    "    a1 = sigmoid(z1)\n",
    "    a1 = np.c_[np.ones((m, 1)), a1]\n",
    "    \n",
    "    z2=  a1.dot(Theta2_l.T)\n",
    "    h = sigmoid(z2)\n",
    "   \n",
    "    J = 0\n",
    "    for k in range(1, num_labels_l + 1):\n",
    "        yk = (y_l==k) * 1\n",
    "        J = J - (1/m) * np.sum(yk * np.log(h.T[k-1:k].T) + (1-yk) * np.log(1-h.T[k-1:k].T))    \n",
    "  \n",
    "    \n",
    "    rtheta1 = np.sum(np.square(Theta1_l[:,1:]))\n",
    "    rtheta2 = np.sum(np.square(Theta2_l[:,1:]))\n",
    "    bias = lambda_l/(2*m)\n",
    "    \n",
    "    J= J + (bias * (rtheta1+rtheta2))\n",
    "    \n",
    "    Del1, Del2 = 0, 0\n",
    "    \n",
    "    for t in range(m): \n",
    "        A1 = X_l[t,:].T.reshape(-1, 1) # all columns with one row at a time\n",
    "        Z2 = Theta1_l.dot(A1)\n",
    "        A2 = np.concatenate((np.c_[np.array([1])], sigmoid(Z2).reshape(-1,1)))\n",
    "        Z3 = Theta2_l.dot(A2)\n",
    "        H = sigmoid(Z3)\n",
    "        actual = y_l[t].reshape(-1,1)\n",
    "        yk = np.zeros((num_labels_l,1))\n",
    "        yk[actual - 1] = 1\n",
    "        del3 = H - yk;\n",
    "        del2 = (Theta2_l[:,1:].T.dot(del3)) * sigmoidGradient(Z2).reshape(-1, 1)\n",
    "       \n",
    "        Del1 = Del1 + del2 * A1.T\n",
    "        Del2 = Del2 + del3 * A2.T\n",
    "    \n",
    "       \n",
    "    Theta1_grad = (Del1/m) + (lambda_l/m) * np.c_[np.zeros((hidden_layer_size_l,1)), Theta1_l[:,1:]]\n",
    "    Theta2_grad = (Del2/m) + (lambda_l/m) * np.c_[np.zeros((num_labels_l,1)), Theta2_l[:,1:]]\n",
    "    \n",
    "    grad = arrangeParams(Theta1_grad, Theta2_grad)\n",
    "    grad = grad.reshape(-1, 1)\n",
    "    print(J, grad.shape)\n",
    "    return J, grad\n",
    "    \n",
    "#calling    \n",
    "lambda_val = 0\n",
    "\n",
    "J, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_val)\n",
    "print(['Cost at parameters (loaded from ex4weights): \\n(this value should be about 0.287629)\\n'], J);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Cost Function (w/ Regularization) ... \n",
      "\n",
      "0.38376985909092365 (10285, 1)\n",
      "['Cost at parameters (loaded from ex4weights):\\n(this value should be about 0.383770)\\n'] 0.38376985909092365\n"
     ]
    }
   ],
   "source": [
    "print('\\nChecking Cost Function (w/ Regularization) ... \\n')\n",
    "lambda_val = 1\n",
    "\n",
    "J, grad = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_val)\n",
    "print(['Cost at parameters (loaded from ex4weights):\\n(this value should be about 0.383770)\\n'], J)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating sigmoid gradient...\n",
      "\n",
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "[0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "print('\\nEvaluating sigmoid gradient...\\n')\n",
    "\n",
    "g = sigmoidGradient(np.array([-1, -0.5, 0, 0.5, 1]))\n",
    "print('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ')\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out):\n",
    "    epsilon_init = 0.12\n",
    "    return np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Neural Network Parameters ...\n",
      "\n",
      "[ 0.10302994 -0.0220073   0.08647456  0.11775641]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ================ Part 6: Initializing Pameters ================\n",
    "#  In this part of the exercise, you will be starting to implment a two\n",
    "#  layer neural network that classifies digits. You will start by\n",
    "#  implementing a function to initialize the weights of the neural network\n",
    "#  (randInitializeWeights.m)\n",
    "np.random.seed(0) #always used before random for testing\n",
    "print('\\nInitializing Neural Network Parameters ...\\n')\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "initial_nn_params = arrangeParams(initial_Theta1, initial_Theta2)\n",
    "print(initial_nn_params[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debugInitializeWeights(fan_out, fan_in):\n",
    "    W = np.zeros((fan_out, 1 + fan_in))\n",
    "    W = (np.sin(np.arange(1, W.size + 1)).reshape(W.shape)) / 10\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeNumericalGradient(J, theta):\n",
    "\n",
    "    numgrad = np.zeros(theta.shape)\n",
    "    perturb = np.zeros(theta.shape)\n",
    "    e = 1e-4\n",
    "    #print(\"---------computeNumericalGradient-----------------\")\n",
    "    #print(theta.size)\n",
    "    #print(\"---------computeNumericalGradient-----------------\")\n",
    "    for p in range(theta.size):\n",
    "        #Set perturbation vector\n",
    "        perturb[p] = e\n",
    "        loss1 = J(theta - perturb)\n",
    "        loss2 = J(theta + perturb)\n",
    "        #Compute Numerical Gradient\n",
    "        numgrad[p] = (loss2 - loss1) / (2*e)\n",
    "        perturb[p] = 0\n",
    "    return numgrad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.exp(0.00001)\n",
    "1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkNNGradients(lamb=0):\n",
    "    input_layer_size = 3\n",
    "    hidden_layer_size = 5\n",
    "    num_labels = 3\n",
    "    m = 5\n",
    "    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)\n",
    "    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)\n",
    "    X  = debugInitializeWeights(m, input_layer_size - 1)\n",
    "    y = np.mod([i for i in range(1,m+1) ], num_labels).reshape(-1,1)\n",
    "    nn_params = arrangeParams(Theta1, Theta2)# np.concatenate((Theta1.T.reshape(Theta1.size,1), Theta2.reshape(Theta2.size,1)))\n",
    "\n",
    "    #print(Theta1)\n",
    "    #print(Theta2)\n",
    "    cost, grad = nnCostFunction(nn_params,input_layer_size,hidden_layer_size, num_labels, X, y, lamb)\n",
    "    \n",
    "    def reduced_cost_func(p): \n",
    "        #print(\"--------------------------------------\")\n",
    "        #print(p)\n",
    "        #print(\"--------------------------------------\")\n",
    "        return nnCostFunction(p,input_layer_size,hidden_layer_size,num_labels,X,y,lamb)[0]\n",
    "     \n",
    "    numgrad = computeNumericalGradient(reduced_cost_func, nn_params)\n",
    "\n",
    "    #print(np.c_[numgrad, grad])\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0993852797173775 (38, 1)\n",
      "2.0993835131753165 (38, 1)\n",
      "2.0993870462561848 (38, 1)\n",
      "2.099384308840557 (38, 1)\n",
      "2.0993862505997076 (38, 1)\n",
      "2.099385997846823 (38, 1)\n",
      "2.0993845615905786 (38, 1)\n",
      "2.099387027996981 (38, 1)\n",
      "2.099383531446314 (38, 1)\n",
      "2.0993864489217042 (38, 1)\n",
      "2.099384110508848 (38, 1)\n",
      "2.0993852313567896 (38, 1)\n",
      "2.0993853280779513 (38, 1)\n",
      "2.099385256046543 (38, 1)\n",
      "2.0993853033882366 (38, 1)\n",
      "2.099385302578475 (38, 1)\n",
      "2.099385256856291 (38, 1)\n",
      "2.0993853280671604 (38, 1)\n",
      "2.0993852313676316 (38, 1)\n",
      "2.0993853090927916 (38, 1)\n",
      "2.0993852503419443 (38, 1)\n",
      "2.099385209040645 (38, 1)\n",
      "2.0993853503940785 (38, 1)\n",
      "2.0993852513520173 (38, 1)\n",
      "2.09938530808279 (38, 1)\n",
      "2.099385319871672 (38, 1)\n",
      "2.099385239563108 (38, 1)\n",
      "2.0993853514222853 (38, 1)\n",
      "2.0993852080125506 (38, 1)\n",
      "2.099385317046896 (38, 1)\n",
      "2.09938524238782 (38, 1)\n",
      "2.0993852517043696 (38, 1)\n",
      "2.0993853077303797 (38, 1)\n",
      "2.099385272736462 (38, 1)\n",
      "2.0993852866983027 (38, 1)\n",
      "2.099385300247191 (38, 1)\n",
      "2.099385259187569 (38, 1)\n",
      "2.0993853088522316 (38, 1)\n",
      "2.0993852505825394 (38, 1)\n",
      "2.099385290680421 (38, 1)\n",
      "2.0993852687543266 (38, 1)\n",
      "2.0993743461947165 (38, 1)\n",
      "2.0993962157391644 (38, 1)\n",
      "2.099374467666789 (38, 1)\n",
      "2.0993960942673047 (38, 1)\n",
      "2.0993346539299615 (38, 1)\n",
      "2.0994359080044003 (38, 1)\n",
      "2.099379600405003 (38, 1)\n",
      "2.099390959708704 (38, 1)\n",
      "2.0993796068608583 (38, 1)\n",
      "2.0993909532529056 (38, 1)\n",
      "2.099358892039411 (38, 1)\n",
      "2.0994116680744273 (38, 1)\n",
      "2.0993800270179857 (38, 1)\n",
      "2.099390532984108 (38, 1)\n",
      "2.0993801355717636 (38, 1)\n",
      "2.0993904244303785 (38, 1)\n",
      "2.099361158453472 (38, 1)\n",
      "2.0994094015487312 (38, 1)\n",
      "2.09937974461278 (38, 1)\n",
      "2.0993908154709224 (38, 1)\n",
      "2.099379797081032 (38, 1)\n",
      "2.0993907630027246 (38, 1)\n",
      "2.099359482331047 (38, 1)\n",
      "2.099411077752781 (38, 1)\n",
      "2.0993796871354107 (38, 1)\n",
      "2.099390872952079 (38, 1)\n",
      "2.0993797107784546 (38, 1)\n",
      "2.0993908493090903 (38, 1)\n",
      "2.09935940685161 (38, 1)\n",
      "2.099411153236005 (38, 1)\n",
      "2.0993800446536803 (38, 1)\n",
      "2.099390515347314 (38, 1)\n",
      "2.0993801620440085 (38, 1)\n",
      "2.099390397957033 (38, 1)\n",
      "2.0993611816218456 (38, 1)\n",
      "2.0994093783792573 (38, 1)\n"
     ]
    }
   ],
   "source": [
    "# print('\\nChecking Backpropagation... \\n');\n",
    "\n",
    "# #Check gradients by running checkNNGradients\n",
    "checkNNGradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Backpropagation (w/ Regularization) ... \n",
      "\n",
      "2.1459566783371105 (38, 1)\n",
      "2.1459549117950494 (38, 1)\n",
      "2.1459584448759177 (38, 1)\n",
      "2.1459557074602897 (38, 1)\n",
      "2.1459576492194405 (38, 1)\n",
      "2.1459573964665557 (38, 1)\n",
      "2.1459559602103115 (38, 1)\n",
      "2.1459584266167138 (38, 1)\n",
      "2.145954930066047 (38, 1)\n",
      "2.145957847541437 (38, 1)\n",
      "2.145955509128581 (38, 1)\n",
      "2.1459511771919613 (38, 1)\n",
      "2.145962185482245 (38, 1)\n",
      "2.145958334159265 (38, 1)\n",
      "2.14595502851498 (38, 1)\n",
      "2.145959968324873 (38, 1)\n",
      "2.1459533943493585 (38, 1)\n",
      "2.145950786042759 (38, 1)\n",
      "2.1459625766314985 (38, 1)\n",
      "2.145961216636005 (38, 1)\n",
      "2.1459521460381965 (38, 1)\n",
      "2.1459557639403295 (38, 1)\n",
      "2.1459575987338595 (38, 1)\n",
      "2.145952711052158 (38, 1)\n",
      "2.145960651622115 (38, 1)\n",
      "2.145962721432644 (38, 1)\n",
      "2.145950641241601 (38, 1)\n",
      "2.145952851314977 (38, 1)\n",
      "2.1459605113593243 (38, 1)\n",
      "2.1459558194033708 (38, 1)\n",
      "2.1459575432708107 (38, 1)\n",
      "2.1459611941390744 (38, 1)\n",
      "2.145952168535141 (38, 1)\n",
      "2.1459507382067153 (38, 1)\n",
      "2.1459626244675154 (38, 1)\n",
      "2.145959921304432 (38, 1)\n",
      "2.1459534413697936 (38, 1)\n",
      "2.1459584378918644 (38, 1)\n",
      "2.145954924782372 (38, 1)\n",
      "2.1459512146286492 (38, 1)\n",
      "2.1459621480455637 (38, 1)\n",
      "2.1459457448144494 (38, 1)\n",
      "2.1459676143588973 (38, 1)\n",
      "2.145945866286522 (38, 1)\n",
      "2.1459674928870376 (38, 1)\n",
      "2.1459060525496945 (38, 1)\n",
      "2.146007306624133 (38, 1)\n",
      "2.1459455462401746 (38, 1)\n",
      "2.145967817112998 (38, 1)\n",
      "2.1459450723311115 (38, 1)\n",
      "2.1459682910221183 (38, 1)\n",
      "2.1459243500150094 (38, 1)\n",
      "2.145989013338294 (38, 1)\n",
      "2.14595058191767 (38, 1)\n",
      "2.145962781323889 (38, 1)\n",
      "2.1459490644805848 (38, 1)\n",
      "2.145964298761023 (38, 1)\n",
      "2.145928658346164 (38, 1)\n",
      "2.145984704895505 (38, 1)\n",
      "2.145955687047485 (38, 1)\n",
      "2.1459576762756836 (38, 1)\n",
      "2.1459544628274303 (38, 1)\n",
      "2.145958900495792 (38, 1)\n",
      "2.1459326113706796 (38, 1)\n",
      "2.1459807519526137 (38, 1)\n",
      "2.1459568423007913 (38, 1)\n",
      "2.145956521026164 (38, 1)\n",
      "2.1459571123394268 (38, 1)\n",
      "2.1459562509875836 (38, 1)\n",
      "2.145936576856294 (38, 1)\n",
      "2.145976786470787 (38, 1)\n",
      "2.1459531227664024 (38, 1)\n",
      "2.145960240474057 (38, 1)\n",
      "2.1459547831012493 (38, 1)\n",
      "2.1459585801392578 (38, 1)\n",
      "2.145937089165059 (38, 1)\n",
      "2.1459762740755095 (38, 1)\n",
      "0.5760512469501331 (10285, 1)\n",
      "['\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): \\n(for lambda = 3, this value should be about 0.576051)\\n\\n'] 3 0.5760512469501331\n"
     ]
    }
   ],
   "source": [
    "print('\\nChecking Backpropagation (w/ Regularization) ... \\n')\n",
    "lambda_val = 3\n",
    "checkNNGradients(lambda_val)\n",
    "debug_J, debug_grad  = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_val);\n",
    "\n",
    "print(['\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): \\n(for lambda = 3, this value should be about 0.576051)\\n\\n'], lambda_val, debug_J)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Neural Network... \n",
      "\n",
      "6.909270510710377 (10285, 1)\n",
      "6.909270510710377 (10285, 1)\n",
      "4.351684303986592 (10285, 1)\n",
      "3.304453113873082 (10285, 1)\n",
      "3.254024171558584 (10285, 1)\n",
      "3.2463980418051968 (10285, 1)\n",
      "3.219764637381702 (10285, 1)\n",
      "3.173006291981995 (10285, 1)\n",
      "3.0332331460088486 (10285, 1)\n",
      "2.9170591525269134 (10285, 1)\n",
      "2.6866924547673654 (10285, 1)\n",
      "2.660525089713059 (10285, 1)\n",
      "2.2842999211248505 (10285, 1)\n",
      "2.297751842111555 (10285, 1)\n",
      "2.049531660488122 (10285, 1)\n",
      "1.930778251952054 (10285, 1)\n",
      "1.8532783110841131 (10285, 1)\n",
      "1.744318214121175 (10285, 1)\n",
      "1.6438132026145353 (10285, 1)\n",
      "1.5854617739813086 (10285, 1)\n",
      "1.542603031819505 (10285, 1)\n",
      "1.4857130217140375 (10285, 1)\n",
      "1.4425446239865014 (10285, 1)\n",
      "1.352208280327473 (10285, 1)\n",
      "1.3159236795453264 (10285, 1)\n",
      "1.2660018676285436 (10285, 1)\n",
      "1.2398195688821008 (10285, 1)\n",
      "1.2029863787560018 (10285, 1)\n",
      "1.174386533577021 (10285, 1)\n",
      "1.1109330536965558 (10285, 1)\n",
      "1.08785502426485 (10285, 1)\n",
      "1.050301434371719 (10285, 1)\n",
      "1.025239039466908 (10285, 1)\n",
      "0.9700129530369588 (10285, 1)\n",
      "0.9471501949728472 (10285, 1)\n",
      "0.9121190670347229 (10285, 1)\n",
      "0.9034036270317682 (10285, 1)\n",
      "0.8766684455879873 (10285, 1)\n",
      "0.8596622695967281 (10285, 1)\n",
      "0.839529942807324 (10285, 1)\n",
      "0.7998105416706125 (10285, 1)\n",
      "0.7873591511207383 (10285, 1)\n",
      "0.7612926607882661 (10285, 1)\n",
      "0.7559200173443711 (10285, 1)\n",
      "0.7420409291688712 (10285, 1)\n",
      "0.7381034823382681 (10285, 1)\n",
      "0.7260760591253175 (10285, 1)\n",
      "0.7192136984159702 (10285, 1)\n",
      "0.7122364685211485 (10285, 1)\n",
      "0.6964238342090348 (10285, 1)\n",
      "0.6889536088028422 (10285, 1)\n",
      "0.6741399365756376 (10285, 1)\n",
      "0.6702836091725041 (10285, 1)\n",
      "0.6617194395749414 (10285, 1)\n",
      "0.6588462850918091 (10285, 1)\n",
      "0.6501410269891237 (10285, 1)\n",
      "0.6455167458251806 (10285, 1)\n",
      "0.6402884761975575 (10285, 1)\n",
      "0.6316464702548565 (10285, 1)\n",
      "0.6278819808749252 (10285, 1)\n",
      "0.6176767650740767 (10285, 1)\n",
      "0.6145138347256391 (10285, 1)\n",
      "0.6074552459085076 (10285, 1)\n",
      "0.6053073393441484 (10285, 1)\n",
      "0.5989417568711246 (10285, 1)\n",
      "0.5961560087270403 (10285, 1)\n",
      "0.5941882202144828 (10285, 1)\n",
      "0.5907294579471742 (10285, 1)\n",
      "0.5887157089840307 (10285, 1)\n",
      "0.5820569338573883 (10285, 1)\n",
      "0.575416578335143 (10285, 1)\n",
      "0.5687564715358528 (10285, 1)\n",
      "0.5521130152526296 (10285, 1)\n",
      "0.5484457876942299 (10285, 1)\n",
      "0.5424386936446091 (10285, 1)\n",
      "0.5404805366324574 (10285, 1)\n",
      "0.5357136807036964 (10285, 1)\n",
      "0.5345822023252012 (10285, 1)\n",
      "0.5320150202497221 (10285, 1)\n",
      "0.5315290405772257 (10285, 1)\n",
      "0.530052510704662 (10285, 1)\n",
      "0.529234147617729 (10285, 1)\n",
      "0.5280244559695431 (10285, 1)\n",
      "0.5241114755593469 (10285, 1)\n",
      "0.5207511267361576 (10285, 1)\n",
      "0.517770080454099 (10285, 1)\n",
      "0.5112470466628694 (10285, 1)\n",
      "0.509428916879159 (10285, 1)\n",
      "0.5043112425662399 (10285, 1)\n",
      "0.5026666239005682 (10285, 1)\n",
      "0.4992872650272623 (10285, 1)\n",
      "0.4901970391865132 (10285, 1)\n",
      "0.4875959910705022 (10285, 1)\n",
      "0.48453045504310505 (10285, 1)\n",
      "0.4837630050249949 (10285, 1)\n",
      "0.48216079441275517 (10285, 1)\n",
      "0.48170873089596666 (10285, 1)\n",
      "0.480352060625734 (10285, 1)\n",
      "0.4796648095045746 (10285, 1)\n",
      "0.4790048497287136 (10285, 1)\n",
      "0.4771401162883595 (10285, 1)\n",
      "0.4765418662294129 (10285, 1)\n",
      "0.4752426330911508 (10285, 1)\n",
      "0.47168761454971686 (10285, 1)\n",
      "0.47079469838141597 (10285, 1)\n",
      "0.46960022024878567 (10285, 1)\n",
      "0.4666075656013541 (10285, 1)\n",
      "0.46551089990126115 (10285, 1)\n",
      "0.46262400183534647 (10285, 1)\n",
      "0.4618273123793564 (10285, 1)\n",
      "0.4592211580700014 (10285, 1)\n",
      "0.4567805728360784 (10285, 1)\n",
      "0.4553650691603819 (10285, 1)\n",
      "0.45268794939731877 (10285, 1)\n",
      "0.4518734100240166 (10285, 1)\n",
      "0.4502330614980184 (10285, 1)\n",
      "0.44978962980010173 (10285, 1)\n",
      "0.44859410683089573 (10285, 1)\n",
      "0.44830617131503075 (10285, 1)\n",
      "0.44745412474627255 (10285, 1)\n",
      "0.44706200111436944 (10285, 1)\n",
      "0.4466493482408743 (10285, 1)\n",
      "0.4454717029073466 (10285, 1)\n",
      "0.4450550008867139 (10285, 1)\n",
      "0.44442949416063443 (10285, 1)\n",
      "0.4429501841636645 (10285, 1)\n",
      "0.44252597879515554 (10285, 1)\n",
      "0.4412178715210606 (10285, 1)\n",
      "0.4404040947221466 (10285, 1)\n",
      "0.4394856171986111 (10285, 1)\n",
      "0.4372190692883925 (10285, 1)\n",
      "0.43657184773413804 (10285, 1)\n",
      "0.4350833335484131 (10285, 1)\n",
      "0.4347772788869869 (10285, 1)\n",
      "0.4340056069156595 (10285, 1)\n",
      "0.43379919382192644 (10285, 1)\n",
      "0.43315802078944093 (10285, 1)\n",
      "0.4327526032692634 (10285, 1)\n",
      "0.43242473281632876 (10285, 1)\n",
      "0.43154152437570664 (10285, 1)\n",
      "0.43113873849083195 (10285, 1)\n",
      "0.43002125836845867 (10285, 1)\n",
      "0.429708152270932 (10285, 1)\n",
      "0.42911848571344446 (10285, 1)\n",
      "0.4277787047067273 (10285, 1)\n",
      "0.42706753285097043 (10285, 1)\n",
      "0.42557745893828564 (10285, 1)\n",
      "0.4252746174323803 (10285, 1)\n",
      "0.4246418319379352 (10285, 1)\n",
      "0.42438980058529263 (10285, 1)\n",
      "0.4235263120369487 (10285, 1)\n",
      "0.42229807729931457 (10285, 1)\n",
      "0.4217606922013464 (10285, 1)\n",
      "0.4203142187696758 (10285, 1)\n",
      "0.4196774586070058 (10285, 1)\n",
      "0.41771946310232366 (10285, 1)\n",
      "0.41655543334309986 (10285, 1)\n",
      "0.4157233776260709 (10285, 1)\n",
      "0.4136693343712342 (10285, 1)\n",
      "0.413013531623302 (10285, 1)\n",
      "0.4116843940734545 (10285, 1)\n",
      "0.41142111755173844 (10285, 1)\n",
      "0.4106614266121832 (10285, 1)\n",
      "0.4103689522129366 (10285, 1)\n",
      "0.410074886752146 (10285, 1)\n",
      "0.4093958675199225 (10285, 1)\n",
      "0.409157339980015 (10285, 1)\n",
      "0.4084484475126875 (10285, 1)\n",
      "0.40811587448903075 (10285, 1)\n",
      "0.40773573519975065 (10285, 1)\n",
      "0.40675171847413916 (10285, 1)\n",
      "0.4063796007819934 (10285, 1)\n",
      "0.4054360323240097 (10285, 1)\n",
      "0.4053108930833127 (10285, 1)\n",
      "0.40494951592818296 (10285, 1)\n",
      "0.40480860046123973 (10285, 1)\n",
      "0.40462174679811763 (10285, 1)\n",
      "0.40408492630088766 (10285, 1)\n",
      "0.40388154295950107 (10285, 1)\n",
      "0.40350569721776325 (10285, 1)\n",
      "0.40222600427177063 (10285, 1)\n",
      "0.4005736076920567 (10285, 1)\n",
      "0.3991699093191564 (10285, 1)\n",
      "0.3955782528069336 (10285, 1)\n",
      "0.3945718254683567 (10285, 1)\n",
      "0.39220962751082467 (10285, 1)\n",
      "0.3918197353661438 (10285, 1)\n",
      "0.3908018592946167 (10285, 1)\n",
      "0.39064038713658944 (10285, 1)\n",
      "0.3901819897915245 (10285, 1)\n",
      "0.39002251518753656 (10285, 1)\n",
      "0.3897310159868275 (10285, 1)\n",
      "0.38899845199393446 (10285, 1)\n",
      "0.3888144286342806 (10285, 1)\n",
      "0.38861356848296724 (10285, 1)\n",
      "0.3885426028559994 (10285, 1)\n",
      "0.3883213429948613 (10285, 1)\n",
      "0.3881751743292577 (10285, 1)\n",
      "0.38804988844703614 (10285, 1)\n",
      "0.3877008262166911 (10285, 1)\n",
      "0.38759827209824205 (10285, 1)\n",
      "0.3873678951249413 (10285, 1)\n",
      "0.38664597099142917 (10285, 1)\n",
      "0.3861586074059229 (10285, 1)\n",
      "0.38542304127599225 (10285, 1)\n",
      "0.3828942611010013 (10285, 1)\n",
      "0.3793624499775913 (10285, 1)\n",
      "0.37774277881595164 (10285, 1)\n",
      "0.37425894238949386 (10285, 1)\n",
      "0.3730376277186673 (10285, 1)\n",
      "0.3701729709394876 (10285, 1)\n",
      "0.3693906578073024 (10285, 1)\n",
      "0.3670905259393054 (10285, 1)\n",
      "0.3660748436318463 (10285, 1)\n",
      "0.36550641167675746 (10285, 1)\n",
      "0.3646604720255649 (10285, 1)\n",
      "0.36438568680856226 (10285, 1)\n",
      "0.3637421612520984 (10285, 1)\n",
      "0.3635559829714473 (10285, 1)\n",
      "0.36305864199509924 (10285, 1)\n",
      "0.3629500487603141 (10285, 1)\n",
      "0.36266211420638583 (10285, 1)\n",
      "0.3625826423290355 (10285, 1)\n",
      "0.36234342463077823 (10285, 1)\n",
      "0.3622198843096839 (10285, 1)\n",
      "0.362064957652707 (10285, 1)\n",
      "0.3618009820863703 (10285, 1)\n",
      "0.36168278622084393 (10285, 1)\n",
      "0.3614856293345442 (10285, 1)\n",
      "0.36134533139872715 (10285, 1)\n",
      "0.3609538983659799 (10285, 1)\n",
      "0.36083275292148415 (10285, 1)\n",
      "0.36073895503587217 (10285, 1)\n",
      "0.3605538148309033 (10285, 1)\n",
      "0.36046940745721134 (10285, 1)\n",
      "0.36025830526000757 (10285, 1)\n",
      "0.36020140784615373 (10285, 1)\n",
      "0.3600163973709955 (10285, 1)\n",
      "0.3598512996201194 (10285, 1)\n",
      "0.35974962790648574 (10285, 1)\n",
      "0.35952711270862514 (10285, 1)\n",
      "0.3594437487754314 (10285, 1)\n",
      "0.35921298001887914 (10285, 1)\n",
      "0.35914760811932805 (10285, 1)\n",
      "0.3590304335529576 (10285, 1)\n",
      "0.35873530635569895 (10285, 1)\n",
      "0.3586181923830258 (10285, 1)\n",
      "0.3583992251063216 (10285, 1)\n",
      "0.35835848606994203 (10285, 1)\n",
      "0.35823194583149065 (10285, 1)\n",
      "0.35815062039046797 (10285, 1)\n",
      "0.35808212451501764 (10285, 1)\n",
      "0.3578839806004981 (10285, 1)\n",
      "0.3578059272694617 (10285, 1)\n",
      "0.357683997183785 (10285, 1)\n",
      "0.35730492028211625 (10285, 1)\n",
      "0.35706039312389093 (10285, 1)\n",
      "0.3567556538389275 (10285, 1)\n",
      "0.3558396316346671 (10285, 1)\n",
      "0.35537591344255115 (10285, 1)\n",
      "0.3546465637253813 (10285, 1)\n",
      "0.35242086593337013 (10285, 1)\n",
      "0.3512337689233255 (10285, 1)\n",
      "0.35056994498510263 (10285, 1)\n",
      "0.3490064067565124 (10285, 1)\n",
      "0.34854061432969524 (10285, 1)\n",
      "0.34755744991985904 (10285, 1)\n",
      "0.3474285134308953 (10285, 1)\n",
      "0.34716611334721537 (10285, 1)\n",
      "0.34706936745377587 (10285, 1)\n",
      "0.3468280407794777 (10285, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34677831903838474 (10285, 1)\n",
      "0.3466072620098289 (10285, 1)\n",
      "0.34635591828646484 (10285, 1)\n",
      "0.3461992538530177 (10285, 1)\n",
      "0.34588471567545365 (10285, 1)\n",
      "0.34578030002101 (10285, 1)\n",
      "0.3455139338716413 (10285, 1)\n",
      "0.3454353454302309 (10285, 1)\n",
      "0.34523390379364416 (10285, 1)\n",
      "0.345212423202087 (10285, 1)\n",
      "0.34514538306645426 (10285, 1)\n",
      "0.34510068971066205 (10285, 1)\n",
      "     fun: 0.34510068971066205\n",
      "     jac: array([ 2.29132935e-04, -2.08080044e-04,  3.10442729e-04, ...,\n",
      "       -6.65223218e-04,  8.11908468e-05,  1.01470393e-04])\n",
      " message: 'Maximum number of iterations has been exceeded.'\n",
      "    nfev: 282\n",
      "     nit: 120\n",
      "    njev: 282\n",
      "  status: 1\n",
      " success: False\n",
      "       x: array([-0.20970948,  0.99400181,  1.45052767, ..., -0.15229443,\n",
      "       -3.07736643,  2.90943555])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#=================== Part 8: Training NN ===================\n",
    "#  You have now implemented all the code necessary to train a neural \n",
    "#  network. To train your neural network, we will now use \"fmincg\", which\n",
    "#  is a function which works similarly to \"fminunc\". Recall that these\n",
    "#  advanced optimizers are able to train our cost functions efficiently as\n",
    "#  long as we provide them with the gradient computations.\n",
    "#\n",
    "print('\\nTraining Neural Network... \\n')\n",
    "lambda_val = 1\n",
    "#print(initial_nn_params.shape)\n",
    "#res = opt.minimize(nnCostFunction, initial_nn_params, args=(input_layer_size, hidden_layer_size, num_labels, X, y, lambda_val), jac=True, options={'maxiter':50})\n",
    "def min_cost_func(p):\n",
    "    j, g = nnCostFunction(p, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_val)\n",
    "    return j, g.flatten()\n",
    "\n",
    "\n",
    "#min_cost_func(initial_nn_params)\n",
    "res = opt.minimize(min_cost_func, initial_nn_params, jac=True, method='CG', options={'maxiter':120, 'eps': 0.8 })\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in test1\n",
      "a =  <function test2.<locals>.test3 at 0x7f19686f9598>\n",
      "b =  20\n",
      "c =  30\n"
     ]
    }
   ],
   "source": [
    "def test1(a, b, c):\n",
    "    print(\"in test1\")\n",
    "    print(\"a = \", a)\n",
    "    print(\"b = \", b)\n",
    "    print(\"c = \", c)\n",
    "\n",
    "def test2():\n",
    "    a1 = 10\n",
    "    b1 = 20\n",
    "    c1 = 30\n",
    "    def test3(d):\n",
    "        print(\"d = \", d)\n",
    "        return \"test3\"\n",
    "    test1(test3, b1, c1)\n",
    "\n",
    "test2()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(a[:,2:3])\n",
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37142028, 0.88655434, 0.37860413, 0.01931643],\n",
       "       [0.14493369, 0.34620824, 0.1663522 , 0.82298146],\n",
       "       [0.99873161, 0.33416302, 0.48518635, 0.04761817],\n",
       "       [0.19446411, 0.22906051, 0.38748291, 0.82641661]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14493369],\n",
       "       [0.34620824],\n",
       "       [0.1663522 ],\n",
       "       [0.82298146]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1 = a[1,:].T.reshape(-1, 1)\n",
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14493369],\n",
       "       [0.34620824],\n",
       "       [0.1663522 ],\n",
       "       [0.82298146]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2 = a[1:2].T\n",
    "A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39921027],\n",
       "       [0.74142861],\n",
       "       [0.52858989],\n",
       "       [0.6636492 ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.random.rand(4, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "print((y[2]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y[2,:].reshape(-1, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
